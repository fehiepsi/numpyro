{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real-world datasets often contain many missing values. In those situations, we have to either remove those missing data (also known as \"complete case\") or replace them by some values. Though using complete case is pretty straightforward, it is only applicable when the number of missing entries is so small that throwing away those entries would not affect much the power of the analysis we are conducting on the data. The second strategy, also known as [imputation](https://en.wikipedia.org/wiki/Imputation_(statistics)), is more applicable and will be our focus in this tutorial.\n",
    "\n",
    "Probably the most popular way to perform imputation is to fill a missing value with the mean, median, or mode of its corresponding feature. In that case, we implicitly assume that the feature containing missing values has no correlation with the remaining features of our dataset. This is a pretty strong assumption and might not be true in general. In addition, it does not encode any uncertainty that we might put on those values. In below, we will construct a *Bayesian* setting to resolve those issues. In particular, given a model on the dataset, we will\n",
    "\n",
    "+ create a generative model for the feature with missing value\n",
    "+ and consider missing values as unobserved latent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we need some imports\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import set_matplotlib_formats\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import ops, random\n",
    "from jax.scipy.special import expit\n",
    "\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.distributions import constraints\n",
    "from numpyro.infer import MCMC, NUTS, Predictive\n",
    "\n",
    "plt.style.use(\"seaborn\")\n",
    "if \"NUMPYRO_SPHINXBUILD\" in os.environ:\n",
    "    set_matplotlib_formats(\"svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is taken from the competition [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic) hosted on [kaggle](https://www.kaggle.com/). It contains information of passengers in the [Titanic accident](https://en.wikipedia.org/wiki/Sinking_of_the_RMS_Titanic) such as name, age, gender,... And our target is to predict if a person is more likely to survive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/agconti/kaggle-titanic/master/data/train.csv\"\n",
    ")\n",
    "train_df.info()\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the data info, we know that there are missing data at `Age`, `Cabin`, and `Embarked` columns. Although `Cabin` is an important feature (because the position of a cabin in the ship can affect the chance of people in that cabin to survive), we will skip it in this tutorial for simplicity. In the datset, there are many categorical columns and two numerical columns `Age` and `Fare`. Let's first look at the distribution of those categorical columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    549\n",
      "1    342\n",
      "Name: Survived, dtype: int64\n",
      "\n",
      "3    491\n",
      "1    216\n",
      "2    184\n",
      "Name: Pclass, dtype: int64\n",
      "\n",
      "male      577\n",
      "female    314\n",
      "Name: Sex, dtype: int64\n",
      "\n",
      "0    608\n",
      "1    209\n",
      "2     28\n",
      "4     18\n",
      "3     16\n",
      "8      7\n",
      "5      5\n",
      "Name: SibSp, dtype: int64\n",
      "\n",
      "0    678\n",
      "1    118\n",
      "2     80\n",
      "5      5\n",
      "3      5\n",
      "4      4\n",
      "6      1\n",
      "Name: Parch, dtype: int64\n",
      "\n",
      "S    644\n",
      "C    168\n",
      "Q     77\n",
      "Name: Embarked, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for col in [\"Survived\", \"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Embarked\"]:\n",
    "    print(train_df[col].value_counts(), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will merge rare groups in `SibSp` and `Parch` columns together. In addition, we'll fill 2 missing entries in `Embarked` by the mode `S`. Note that we can make a generative model for those missing entries in `Embarked` but let's skip doing so for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = train_df.copy()\n",
    "d.SibSp.clip(0, 1, inplace=True)\n",
    "d.Parch.clip(0, 2, inplace=True)\n",
    "d.Embarked.fillna(\"S\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking closer at the data, we can observe that each name contains a title. We know that age is correlated with the title of the name: e.g. those with Mrs. would be older than those with `Miss.` (in average) so it might be good to create that feature. The distribution of titles is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mr.          517\n",
       "Miss.        182\n",
       "Mrs.         125\n",
       "Master.       40\n",
       "Dr.            7\n",
       "Rev.           6\n",
       "Major.         2\n",
       "Col.           2\n",
       "Mlle.          2\n",
       "the            1\n",
       "Capt.          1\n",
       "Lady.          1\n",
       "Mme.           1\n",
       "Ms.            1\n",
       "Sir.           1\n",
       "Don.           1\n",
       "Jonkheer.      1\n",
       "Name: Name, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.Name.str.split(\", \").str.get(1).str.split(\" \").str.get(0).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make a new column `Title`, where rare titles are merged into one group `Misc.`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[\"Title\"] = (\n",
    "    d.Name.str.split(\", \")\n",
    "    .str.get(1)\n",
    "    .str.split(\" \")\n",
    "    .str.get(0)\n",
    "    .apply(lambda x: x if x in [\"Mr.\", \"Miss.\", \"Mrs.\", \"Master.\"] else \"Misc.\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is ready to turn the dataframe, which includes categorical values, into numpy arrays. We also perform standardization (a good practice for regression models) for `Age` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_cat = pd.CategoricalDtype(\n",
    "    categories=[\"Mr.\", \"Miss.\", \"Mrs.\", \"Master.\", \"Misc.\"], ordered=True\n",
    ")\n",
    "embarked_cat = pd.CategoricalDtype(categories=[\"S\", \"C\", \"Q\"], ordered=True)\n",
    "age_mean, age_std = d.Age.mean(), d.Age.std()\n",
    "data = dict(\n",
    "    age=d.Age.pipe(lambda x: (x - age_mean) / age_std).values,\n",
    "    pclass=d.Pclass.values - 1,\n",
    "    title=d.Title.astype(title_cat).cat.codes.values,\n",
    "    sex=(d.Sex == \"male\").astype(int).values,\n",
    "    sibsp=d.SibSp.values,\n",
    "    parch=d.Parch.values,\n",
    "    embarked=d.Embarked.astype(embarked_cat).cat.codes.values,\n",
    "    survived=d.Survived.values,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we would like to talk a bit about how to define improper priors in NumPyro. Consider the following logistic regression model,\n",
    "```python\n",
    "def model(x, y):\n",
    "    a = numpyro.sample(\"a\", dist.Normal(0, 1))\n",
    "    b = numpyro.param(\"b\", dist.Normal(0, 1))\n",
    "    sigma = numpyro.sample(\"sigma\", dist.Exponential(1))\n",
    "    numpyro.sample(\"obs\", dist.Normal(a + b * x, sigma), obs=y)\n",
    "```\n",
    ". There we have some priors on sites `a`, `b`, and `sigma`. Now, assume that there is no apriori information on those sites, how can we define and get posterior samples of `a`, `b`, and `sigma`? In NumPyro, we can do it using [ImproperUniform](http://num.pyro.ai/en/latest/distributions.html#improperuniform) distribution. For example, a corresponding linear regression model with no prior information is\n",
    "```python\n",
    "def model(x, y):\n",
    "    a = numpyro.sample(\"a\", dist.ImproperUniform(constraints.real, (), ()))\n",
    "    b = numpyro.sample(\"b\", dist.ImproperUniform(constraints.real, (), ()))\n",
    "    sigma = numpyro.sample(\"sigma\", dist.ImproperUniform(constraints.positive, (), ()))\n",
    "    numpyro.sample(\"obs\", dist.Normal(a + b * x, sigma), obs=y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another remark is: in MCMC context, the following models\n",
    "```python\n",
    "def model1():\n",
    "    a = numpyro.sample(\"a\", dist.Normal(0, 1))\n",
    "```\n",
    "and\n",
    "```python\n",
    "def model2():\n",
    "    a = numpyro.sample(\"a\", dist.ImproperUniform(constraints.real, (), ()))\n",
    "    numpyro.sample(\"a_obs\", dist.Normal(0, 1), obs=a)\n",
    "```\n",
    "are equivalent because both of them have\n",
    "+ the same latent sites `a`,\n",
    "+ and the same log densities `dist.Normal(0, 1).log_prob(a)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With those remarks in mind, we are ready to define a logistic regression model to predict survival chance given passengers' information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(age, pclass, title, sex, sibsp, parch, embarked, survived=None):\n",
    "    b_pclass = numpyro.sample(\"b_Pclass\", dist.Normal(0, 1), sample_shape=(3,))\n",
    "    b_title = numpyro.sample(\"b_Title\", dist.Normal(0, 1), sample_shape=(5,))\n",
    "    b_sex = numpyro.sample(\"b_Sex\", dist.Normal(0, 1), sample_shape=(2,))\n",
    "    b_sibsp = numpyro.sample(\"b_SibSp\", dist.Normal(0, 1), sample_shape=(2,))\n",
    "    b_parch = numpyro.sample(\"b_Parch\", dist.Normal(0, 1), sample_shape=(3,))\n",
    "    b_embarked = numpyro.sample(\"b_Embarked\", dist.Normal(0, 1), sample_shape=(3,))\n",
    "\n",
    "    # impute age by Title\n",
    "    age_mu = numpyro.sample(\"age_mu\", dist.Normal(0, 1), sample_shape=(5,))\n",
    "    age_mu = age_mu[title]\n",
    "    age_sigma = numpyro.sample(\"age_sigma\", dist.Normal(0, 1), sample_shape=(5,))\n",
    "    age_sigma = age_sigma[title]\n",
    "    age_isnan = jnp.isnan(age)\n",
    "    age_nanidx = jnp.nonzero(age_isnan)[0]\n",
    "    if survived is not None:\n",
    "        age_impute = numpyro.sample(\n",
    "            \"age_impute\",\n",
    "            dist.ImproperUniform(constraints.real, (), ()),\n",
    "            sample_shape=(age_isnan.sum(),),\n",
    "        )\n",
    "    else:  # we are making prediction\n",
    "        age_impute = numpyro.sample(\n",
    "            \"age_impute\", dist.Normal(age_mu[age_nanidx], age_sigma[age_nanidx])\n",
    "        )\n",
    "    age = ops.index_update(age, age_nanidx, age_impute)\n",
    "    numpyro.sample(\"age\", dist.Normal(age_mu, age_sigma), obs=age)\n",
    "\n",
    "    a = numpyro.sample(\"a\", dist.Normal(0, 1))\n",
    "    b_age = numpyro.sample(\"b_Age\", dist.Normal(0, 1))\n",
    "    logits = a + b_age * age\n",
    "\n",
    "    logits = (\n",
    "        logits\n",
    "        + b_title[title]\n",
    "        + b_pclass[pclass]\n",
    "        + b_sex[sex]\n",
    "        + b_sibsp[sibsp]\n",
    "        + b_parch[parch]\n",
    "        + b_embarked[embarked]\n",
    "    )\n",
    "    if survived is None:  # record `probs` value in prediction\n",
    "        numpyro.deterministic(\"probs\", expit(logits))\n",
    "    numpyro.sample(\"survived\", dist.Bernoulli(logits=logits), obs=survived)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the model, the prior for `age` is `dist.Normal(age_mu, age_sigma)`, where the values of `age_mu` and `age_sigma` depend on `title`. Because there are missing values in `age`, we will encode those missing values in the latent parameter `age_impute`. Then we can replace `NaN` entries in `age` with the vector `age_impute`. Under the hood, similar to `model2` in the above remark, `age_impute` will have prior `dist.Normal(age_mu, age_sigma)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use MCMC with NUTS kernel to sample both regression coefficients and imputed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 2000/2000 [00:20<00:00, 97.78it/s, 63 steps of size 6.78e-02. acc. prob=0.93] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
      "              a      0.16      0.82      0.16     -1.24      1.42   1245.25      1.00\n",
      "  age_impute[0]      0.24      0.83      0.28     -1.13      1.58   2586.24      1.00\n",
      "  age_impute[1]     -0.09      0.86     -0.08     -1.43      1.38   2008.07      1.00\n",
      "  age_impute[2]      0.36      0.79      0.33     -0.96      1.61   1409.93      1.00\n",
      "  age_impute[3]      0.21      0.89      0.23     -1.24      1.67   2565.59      1.00\n",
      "  age_impute[4]     -0.65      0.90     -0.63     -2.07      0.92   2164.18      1.00\n",
      "  age_impute[5]      0.23      0.87      0.22     -1.12      1.65   2028.33      1.00\n",
      "  age_impute[6]      0.44      0.80      0.44     -0.87      1.79   2666.55      1.00\n",
      "  age_impute[7]     -0.66      0.91     -0.65     -2.05      0.97   2164.02      1.00\n",
      "  age_impute[8]     -0.08      0.89     -0.09     -1.56      1.33   1493.14      1.00\n",
      "  age_impute[9]      0.24      0.91      0.25     -1.30      1.79   1550.50      1.00\n",
      " age_impute[10]      0.22      0.86      0.22     -1.23      1.55   2555.96      1.00\n",
      " age_impute[11]      0.17      0.85      0.19     -1.27      1.47   1788.85      1.00\n",
      " age_impute[12]     -0.64      0.91     -0.64     -2.10      0.85   1963.00      1.00\n",
      " age_impute[13]      0.20      0.91      0.18     -1.29      1.61   2705.10      1.00\n",
      " age_impute[14]     -0.02      0.87     -0.01     -1.43      1.41   2286.11      1.00\n",
      " age_impute[15]      0.38      0.84      0.39     -1.05      1.67   1536.08      1.00\n",
      " age_impute[16]     -1.74      0.25     -1.74     -2.13     -1.32   2398.45      1.00\n",
      " age_impute[17]      0.21      0.87      0.22     -1.15      1.61   1902.30      1.00\n",
      " age_impute[18]      0.23      0.93      0.25     -1.36      1.70   1786.59      1.00\n",
      " age_impute[19]     -0.67      0.87     -0.64     -2.18      0.73   2159.56      1.00\n",
      " age_impute[20]      0.21      0.93      0.24     -1.24      1.81   1988.41      1.00\n",
      " age_impute[21]      0.19      0.89      0.23     -1.27      1.55   2210.75      1.00\n",
      " age_impute[22]      0.19      0.91      0.19     -1.34      1.64   1901.05      1.00\n",
      " age_impute[23]     -0.15      0.87     -0.14     -1.62      1.20   2104.69      1.00\n",
      " age_impute[24]     -0.69      0.92     -0.68     -2.23      0.74   2151.08      1.00\n",
      " age_impute[25]      0.16      0.87      0.18     -1.20      1.63   1705.91      1.00\n",
      " age_impute[26]      0.20      0.84      0.23     -1.21      1.53   1640.32      1.00\n",
      " age_impute[27]     -0.71      0.91     -0.71     -2.18      0.77   2322.71      1.00\n",
      " age_impute[28]      0.60      0.75      0.63     -0.60      1.77   1843.93      1.00\n",
      " age_impute[29]      0.22      0.85      0.20     -1.27      1.57   2957.58      1.00\n",
      " age_impute[30]      0.23      0.85      0.22     -1.08      1.63   2677.75      1.00\n",
      " age_impute[31]     -1.71      0.27     -1.72     -2.10     -1.21   2743.72      1.00\n",
      " age_impute[32]      0.43      0.85      0.45     -0.97      1.81   1540.82      1.00\n",
      " age_impute[33]      0.32      0.85      0.28     -1.06      1.69   2294.27      1.00\n",
      " age_impute[34]     -1.72      0.28     -1.73     -2.20     -1.27   2587.75      1.00\n",
      " age_impute[35]     -0.42      0.87     -0.41     -2.01      0.81   1742.01      1.00\n",
      " age_impute[36]      0.28      0.86      0.29     -1.19      1.61   1997.53      1.00\n",
      " age_impute[37]      0.30      0.83      0.33     -0.92      1.83   2293.61      1.00\n",
      " age_impute[38]      0.34      0.78      0.34     -0.82      1.71   2401.14      1.00\n",
      " age_impute[39]      0.18      0.92      0.18     -1.24      1.80   1322.22      1.00\n",
      " age_impute[40]     -0.66      0.89     -0.66     -2.05      0.77   1823.56      1.00\n",
      " age_impute[41]      0.18      0.84      0.19     -1.43      1.40   2463.71      1.00\n",
      " age_impute[42]      0.24      0.88      0.25     -1.25      1.62   2140.17      1.00\n",
      " age_impute[43]      0.21      0.82      0.24     -1.13      1.55   1741.40      1.00\n",
      " age_impute[44]     -0.40      0.93     -0.39     -1.88      1.15   1622.82      1.00\n",
      " age_impute[45]     -0.33      0.88     -0.32     -1.78      1.09   1545.22      1.00\n",
      " age_impute[46]     -0.31      0.90     -0.33     -1.88      1.10   1636.27      1.00\n",
      " age_impute[47]     -0.70      0.91     -0.70     -2.26      0.69   2421.54      1.00\n",
      " age_impute[48]      0.21      0.89      0.21     -1.22      1.71   1827.38      1.00\n",
      " age_impute[49]      0.43      0.77      0.45     -0.81      1.69   1925.91      1.00\n",
      " age_impute[50]      0.25      0.85      0.24     -1.08      1.60   1822.18      1.00\n",
      " age_impute[51]     -0.30      0.88     -0.32     -1.63      1.20   2241.01      1.00\n",
      " age_impute[52]      0.36      0.82      0.36     -0.96      1.71   2302.90      1.00\n",
      " age_impute[53]     -0.66      0.93     -0.64     -2.29      0.84   2165.71      1.00\n",
      " age_impute[54]      0.25      0.88      0.27     -1.33      1.49   2383.03      1.00\n",
      " age_impute[55]      0.33      0.87      0.33     -0.96      1.86   1869.82      1.00\n",
      " age_impute[56]      0.38      0.79      0.39     -1.04      1.54   1529.90      1.00\n",
      " age_impute[57]      0.00      0.80     -0.02     -1.34      1.30   2421.47      1.00\n",
      " age_impute[58]     -0.68      0.89     -0.67     -2.11      0.77   1634.02      1.00\n",
      " age_impute[59]     -0.13      0.89     -0.12     -1.64      1.20   1500.87      1.00\n",
      " age_impute[60]     -0.62      0.87     -0.58     -2.00      0.82   2128.74      1.00\n",
      " age_impute[61]      0.21      0.84      0.23     -1.22      1.56   2513.99      1.00\n",
      " age_impute[62]     -0.58      0.92     -0.57     -1.98      1.03   1743.63      1.00\n",
      " age_impute[63]      0.21      0.88      0.22     -1.19      1.65   1688.70      1.00\n",
      " age_impute[64]     -0.70      0.91     -0.66     -2.23      0.78   2030.86      1.00\n",
      " age_impute[65]      0.40      0.74      0.42     -0.84      1.62   1356.12      1.00\n",
      " age_impute[66]      0.24      0.94      0.21     -1.18      1.89   2818.80      1.00\n",
      " age_impute[67]      0.33      0.73      0.32     -0.81      1.58   1728.30      1.00\n",
      " age_impute[68]      0.33      0.87      0.32     -0.94      1.95   2004.02      1.00\n",
      " age_impute[69]      0.23      0.94      0.22     -1.29      1.70   1422.91      1.00\n",
      " age_impute[70]     -0.66      0.86     -0.63     -2.14      0.66   1788.58      1.00\n",
      " age_impute[71]     -0.68      0.85     -0.70     -2.16      0.62   2117.80      1.00\n",
      " age_impute[72]      0.19      0.85      0.16     -1.19      1.60   2170.50      1.00\n",
      " age_impute[73]      0.38      0.77      0.38     -0.74      1.77   1884.55      1.00\n",
      " age_impute[74]     -0.67      0.89     -0.66     -2.01      0.89   1555.52      1.00\n",
      " age_impute[75]      0.42      0.84      0.40     -0.89      1.84   1572.40      1.00\n",
      " age_impute[76]      0.20      0.86      0.21     -1.24      1.53   2927.68      1.00\n",
      " age_impute[77]      0.19      0.86      0.19     -1.24      1.51   1905.19      1.00\n",
      " age_impute[78]     -0.42      0.88     -0.40     -1.91      0.97   1974.72      1.00\n",
      " age_impute[79]      0.20      0.84      0.20     -1.18      1.57   2521.30      1.00\n",
      " age_impute[80]      0.24      0.85      0.26     -1.22      1.58   1827.72      1.00\n",
      " age_impute[81]      0.26      0.87      0.27     -1.06      1.79   2861.22      1.00\n",
      " age_impute[82]      0.62      0.82      0.61     -0.61      2.00   1460.65      1.00\n",
      " age_impute[83]      0.21      0.84      0.21     -1.08      1.69   2188.02      1.00\n",
      " age_impute[84]      0.21      0.86      0.21     -1.31      1.58   1412.42      1.00\n",
      " age_impute[85]      0.26      0.85      0.24     -1.02      1.77   2584.05      1.00\n",
      " age_impute[86]      0.32      0.78      0.33     -0.96      1.58   1863.32      1.00\n",
      " age_impute[87]     -0.14      0.87     -0.13     -1.47      1.39   2134.81      1.00\n",
      " age_impute[88]      0.21      0.90      0.23     -1.39      1.53   1753.33      1.00\n",
      " age_impute[89]      0.23      0.93      0.22     -1.05      1.93   3708.57      1.00\n",
      " age_impute[90]      0.40      0.81      0.38     -0.92      1.76   2073.74      1.00\n",
      " age_impute[91]      0.25      0.94      0.27     -1.29      1.77   2319.92      1.00\n",
      " age_impute[92]      0.20      0.86      0.19     -1.25      1.51   2083.08      1.00\n",
      " age_impute[93]      0.24      0.88      0.26     -1.12      1.76   2033.56      1.00\n",
      " age_impute[94]      0.21      0.90      0.16     -1.22      1.71   2580.44      1.00\n",
      " age_impute[95]      0.22      0.91      0.20     -1.18      1.75   1771.56      1.00\n",
      " age_impute[96]      0.35      0.92      0.32     -1.10      1.83   2079.93      1.00\n",
      " age_impute[97]      0.28      0.86      0.29     -1.08      1.73   2084.46      1.00\n",
      " age_impute[98]     -0.39      0.92     -0.40     -1.88      1.05   1925.32      1.00\n",
      " age_impute[99]      0.16      0.93      0.13     -1.26      1.70   1676.54      1.00\n",
      "age_impute[100]      0.24      0.88      0.22     -1.40      1.48   1734.88      1.00\n",
      "age_impute[101]      0.20      0.87      0.20     -1.23      1.61   2228.05      1.00\n",
      "age_impute[102]     -0.31      0.91     -0.30     -1.67      1.24   2085.58      1.00\n",
      "age_impute[103]     -0.01      0.85     -0.03     -1.28      1.54   2224.10      1.00\n",
      "age_impute[104]      0.26      0.90      0.29     -1.39      1.56   1822.14      1.00\n",
      "age_impute[105]      0.25      0.88      0.26     -1.22      1.66   1894.29      1.00\n",
      "age_impute[106]      0.25      0.87      0.23     -1.25      1.62   3146.76      1.00\n",
      "age_impute[107]      0.20      0.86      0.21     -1.15      1.66   2146.91      1.00\n",
      "age_impute[108]      0.34      0.83      0.36     -1.11      1.60   1753.22      1.00\n",
      "age_impute[109]      0.27      0.88      0.23     -1.06      1.81   1534.56      1.00\n",
      "age_impute[110]      0.32      0.77      0.36     -0.95      1.52   1705.99      1.00\n",
      "age_impute[111]      0.22      0.84      0.20     -1.08      1.66   2456.52      1.00\n",
      "age_impute[112]     -0.02      0.87     -0.02     -1.45      1.42   1958.71      1.00\n",
      "age_impute[113]      0.23      0.88      0.24     -1.13      1.79   1510.25      1.00\n",
      "age_impute[114]      0.39      0.81      0.37     -0.96      1.68   1635.99      1.00\n",
      "age_impute[115]      0.20      0.87      0.19     -1.23      1.63   1613.88      1.01\n",
      "age_impute[116]      0.25      0.86      0.22     -1.20      1.53   1401.08      1.00\n",
      "age_impute[117]     -0.35      0.90     -0.34     -1.93      1.03   2147.49      1.00\n",
      "age_impute[118]      0.23      0.95      0.24     -1.30      1.87   3027.20      1.00\n",
      "age_impute[119]     -0.63      0.95     -0.67     -2.19      0.89   1614.50      1.00\n",
      "age_impute[120]      0.60      0.80      0.60     -0.53      2.04   1575.05      1.00\n",
      "age_impute[121]      0.20      0.89      0.23     -1.21      1.64   2117.62      1.00\n",
      "age_impute[122]      0.22      0.82      0.18     -1.28      1.36   2296.25      1.00\n",
      "age_impute[123]     -0.38      0.95     -0.41     -1.95      1.19   1581.75      1.00\n",
      "age_impute[124]     -0.59      0.94     -0.59     -2.17      0.93   1677.39      1.00\n",
      "age_impute[125]      0.24      0.91      0.22     -1.21      1.69   2146.46      1.00\n",
      "age_impute[126]      0.21      0.83      0.20     -1.23      1.51   2283.99      1.00\n",
      "age_impute[127]      0.36      0.85      0.35     -0.94      1.81   2553.12      1.00\n",
      "age_impute[128]      0.25      0.91      0.29     -1.24      1.69   1651.53      1.00\n",
      "age_impute[129]     -0.71      0.90     -0.69     -2.07      0.85   2127.58      1.00\n",
      "age_impute[130]      0.19      0.85      0.17     -1.29      1.46   2185.88      1.00\n",
      "age_impute[131]      0.23      0.89      0.24     -1.21      1.67   2229.93      1.00\n",
      "age_impute[132]      0.31      0.88      0.29     -1.19      1.71   2369.68      1.00\n",
      "age_impute[133]      0.23      0.90      0.21     -1.12      1.74   1457.53      1.00\n",
      "age_impute[134]     -0.12      0.93     -0.16     -1.53      1.41   2282.20      1.00\n",
      "age_impute[135]      0.22      0.84      0.24     -1.07      1.74   1950.07      1.00\n",
      "age_impute[136]      0.19      0.85      0.19     -1.17      1.59   2689.67      1.00\n",
      "age_impute[137]     -0.68      0.90     -0.64     -2.22      0.76   2181.09      1.00\n",
      "age_impute[138]      0.19      0.94      0.18     -1.41      1.65   1958.06      1.00\n",
      "age_impute[139]      0.19      0.83      0.19     -1.13      1.61   1739.59      1.00\n",
      "age_impute[140]      0.39      0.78      0.38     -0.91      1.59   2192.48      1.00\n",
      "age_impute[141]      0.25      0.89      0.26     -1.23      1.59   1951.71      1.00\n",
      "age_impute[142]     -0.31      0.92     -0.32     -1.78      1.22   2244.74      1.00\n",
      "age_impute[143]     -0.15      0.85     -0.14     -1.51      1.14   2161.58      1.00\n",
      "age_impute[144]     -0.68      0.94     -0.68     -2.26      0.72   2216.52      1.00\n",
      "age_impute[145]     -1.75      0.25     -1.74     -2.18     -1.35   2062.83      1.00\n",
      "age_impute[146]      0.34      0.86      0.35     -0.99      1.74   2279.86      1.00\n",
      "age_impute[147]      0.25      0.86      0.27     -1.28      1.54   2776.97      1.00\n",
      "age_impute[148]     -0.67      0.88     -0.66     -2.10      0.76   2315.78      1.00\n",
      "age_impute[149]      0.28      0.83      0.28     -1.06      1.60   1820.67      1.00\n",
      "age_impute[150]      0.20      0.89      0.23     -1.30      1.56   2593.48      1.00\n",
      "age_impute[151]      0.21      0.87      0.19     -1.16      1.66   1729.75      1.00\n",
      "age_impute[152]      0.01      0.86     -0.00     -1.52      1.34   2816.59      1.00\n",
      "age_impute[153]      0.19      0.89      0.23     -1.36      1.58   2203.93      1.00\n",
      "age_impute[154]      1.06      0.96      1.07     -0.47      2.65   1731.83      1.00\n",
      "age_impute[155]      0.22      0.83      0.23     -1.08      1.56   1806.10      1.00\n",
      "age_impute[156]      0.26      0.94      0.27     -1.30      1.78   1560.04      1.00\n",
      "age_impute[157]      0.22      0.91      0.20     -1.25      1.80   1518.76      1.00\n",
      "age_impute[158]      0.23      0.87      0.22     -1.13      1.71   1660.13      1.00\n",
      "age_impute[159]      0.21      0.90      0.23     -1.13      1.71   1828.73      1.00\n",
      "age_impute[160]      0.20      0.85      0.19     -1.23      1.52   1595.02      1.00\n",
      "age_impute[161]     -0.49      0.91     -0.50     -2.03      0.94   1652.39      1.00\n",
      "age_impute[162]      0.36      0.84      0.35     -0.93      1.83   2238.71      1.00\n",
      "age_impute[163]      0.31      0.85      0.29     -1.10      1.66   1898.82      1.00\n",
      "age_impute[164]      0.22      0.87      0.22     -1.29      1.64   2365.04      1.00\n",
      "age_impute[165]      0.23      0.86      0.24     -1.06      1.54   2196.56      1.00\n",
      "age_impute[166]     -0.10      0.87     -0.10     -1.46      1.35   2108.08      1.00\n",
      "age_impute[167]      0.23      0.90      0.23     -1.11      1.76   1835.03      1.00\n",
      "age_impute[168]      0.20      0.84      0.19     -1.26      1.47   2068.07      1.00\n",
      "age_impute[169]      0.01      0.89      0.01     -1.49      1.38   2389.25      1.00\n",
      "age_impute[170]      0.15      0.89      0.15     -1.31      1.53   1983.34      1.00\n",
      "age_impute[171]      0.42      0.84      0.44     -0.85      1.85   2362.59      1.00\n",
      "age_impute[172]      0.25      0.88      0.22     -1.27      1.54   2026.27      1.00\n",
      "age_impute[173]     -0.44      0.88     -0.46     -1.79      1.02   2604.42      1.00\n",
      "age_impute[174]      0.22      0.83      0.24     -1.07      1.59   2626.77      1.00\n",
      "age_impute[175]      0.23      0.94      0.26     -1.37      1.76   2601.98      1.00\n",
      "age_impute[176]     -0.43      0.89     -0.42     -1.80      1.06   2353.81      1.00\n",
      "      age_mu[0]      0.19      0.04      0.19      0.11      0.26   1661.98      1.00\n",
      "      age_mu[1]     -0.55      0.08     -0.55     -0.66     -0.41   1198.59      1.00\n",
      "      age_mu[2]      0.42      0.08      0.42      0.29      0.55   1319.36      1.00\n",
      "      age_mu[3]     -1.73      0.04     -1.73     -1.79     -1.65   1223.88      1.00\n",
      "      age_mu[4]      0.85      0.19      0.85      0.57      1.17   1372.65      1.00\n",
      "   age_sigma[0]      0.88      0.03      0.88      0.83      0.93   1040.64      1.00\n",
      "   age_sigma[1]      0.90      0.05      0.90      0.81      0.98   1098.46      1.00\n",
      "   age_sigma[2]      0.79      0.05      0.79      0.70      0.88   1024.36      1.00\n",
      "   age_sigma[3]      0.26      0.03      0.25      0.21      0.31   1604.62      1.00\n",
      "   age_sigma[4]      0.94      0.14      0.93      0.74      1.17   1099.01      1.00\n",
      "          b_Age     -0.44      0.13     -0.44     -0.64     -0.23    994.55      1.00\n",
      "  b_Embarked[0]     -0.30      0.53     -0.30     -1.15      0.56    697.67      1.00\n",
      "  b_Embarked[1]      0.27      0.54      0.28     -0.72      1.07    704.89      1.00\n",
      "  b_Embarked[2]      0.02      0.55      0.00     -0.95      0.82    549.84      1.00\n",
      "     b_Parch[0]      0.46      0.57      0.48     -0.41      1.46    625.96      1.00\n",
      "     b_Parch[1]      0.10      0.57      0.09     -0.81      1.04    640.32      1.00\n",
      "     b_Parch[2]     -0.48      0.57     -0.47     -1.28      0.57    628.88      1.00\n",
      "    b_Pclass[0]      1.17      0.56      1.16      0.27      2.08    533.19      1.00\n",
      "    b_Pclass[1]      0.02      0.56      0.02     -0.83      1.00    550.01      1.00\n",
      "    b_Pclass[2]     -1.22      0.56     -1.20     -2.12     -0.28    541.13      1.00\n",
      "       b_Sex[0]      1.16      0.71      1.15     -0.00      2.30    807.53      1.00\n",
      "       b_Sex[1]     -1.01      0.69     -1.03     -2.17      0.06   1001.63      1.00\n",
      "     b_SibSp[0]      0.27      0.62      0.28     -0.76      1.24    862.20      1.00\n",
      "     b_SibSp[1]     -0.18      0.64     -0.18     -1.24      0.80    852.57      1.00\n",
      "     b_Title[0]     -0.96      0.57     -0.97     -1.85     -0.00    596.91      1.00\n",
      "     b_Title[1]     -0.34      0.62     -0.34     -1.36      0.67    728.62      1.00\n",
      "     b_Title[2]      0.54      0.63      0.54     -0.50      1.53    671.43      1.00\n",
      "     b_Title[3]      1.46      0.63      1.47      0.39      2.46    767.41      1.00\n",
      "     b_Title[4]     -0.67      0.61     -0.69     -1.61      0.44    764.05      1.00\n",
      "\n",
      "Number of divergences: 0\n"
     ]
    }
   ],
   "source": [
    "mcmc = MCMC(NUTS(model), 1000, 1000)\n",
    "mcmc.run(random.PRNGKey(0), **data)\n",
    "mcmc.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To double check that the assumption \"age is correlated with title\" is reasonable, let's look at the infered age by title. Recall that we performed standarization on `age`, so here we need to scale back to original domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Mr.': 32.431396,\n",
       " 'Miss.': 21.763844,\n",
       " 'Mrs.': 35.838753,\n",
       " 'Master.': 4.6307297,\n",
       " 'Misc.': 42.057827}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_by_title = age_mean + age_std * mcmc.get_samples()[\"age_mu\"].mean(axis=0)\n",
    "dict(zip(title_cat.categories, age_by_title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The infered result confirms our assumption that `Age` is correlated with `Title`:\n",
    "\n",
    "+ those with `Master.` title has pretty small age (in other words, they are children in the ship) comparing to the other groups,\n",
    "+ those with `Mrs.` title have larger age than those with `Miss.` title (in average).\n",
    "\n",
    "We can also see that the result is similar to the actual statistical mean of `Age` given `Title` in our training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Title\n",
       "Master.     4.574167\n",
       "Misc.      42.384615\n",
       "Miss.      21.773973\n",
       "Mr.        32.368090\n",
       "Mrs.       35.898148\n",
       "Name: Age, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.groupby(\"Title\")[\"Age\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good, we have many information about the regression coefficients together with imputed values and their uncertainties. Let's inspect those results a bit:\n",
    "\n",
    "+ The mean value `-0.44` of `b_Age` implies that those with smaller ages have better chance to survive.\n",
    "+ The mean value `(1.11, -1.07)` of `b_Sex` implies that female passengers have higher chance to survive than male passengers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NumPyro, we can use [Predictive](http://num.pyro.ai/en/stable/utilities.html#numpyro.infer.util.Predictive) utility for making predictions from posterior samples. Let's check how well the model performs on the training dataset. In this case, we will predict the chance to survive of each passenger (in other words, the `probs` values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(0.8260382, dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posterior = mcmc.get_samples().copy()\n",
    "survived = data.pop(\"survived\")\n",
    "survived_probs = Predictive(model, posterior)(random.PRNGKey(1), **data)[\"probs\"]\n",
    "(\n",
    "    (survived_probs.mean(axis=0) >= 0.5).astype(jnp.uint8) == survived\n",
    ").sum() / survived.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pretty good result using a simple logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes to the test set, which also contains missing data in `Age` feature. We can't use `age_impute` from posterior samples because it is specific to the training set. So we need to marginalize them from the joint posterior distribution first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 0.22152053, -0.61639136,  1.7305413 , ..., -1.1393623 ,\n",
       "               0.50946575, -1.0020618 ],\n",
       "             [ 0.31746617,  0.29219964, -0.8941262 , ...,  1.5754844 ,\n",
       "              -0.7020866 ,  0.43354604],\n",
       "             [-0.38118613, -0.01952589,  1.7682728 , ..., -1.2500907 ,\n",
       "               1.3128119 , -1.3545763 ],\n",
       "             ...,\n",
       "             [-0.35086122, -0.61043584,  0.48523673, ...,  1.2864797 ,\n",
       "              -0.57773304, -0.3193913 ],\n",
       "             [ 0.60243225,  0.7971147 ,  0.43930596, ..., -0.16035162,\n",
       "               0.81113493,  0.09520832],\n",
       "             [-0.2894233 , -0.8771592 ,  0.57905334, ...,  0.5363015 ,\n",
       "              -0.27452654, -0.9621993 ]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posterior.pop(\"age_impute\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will use the generative model for `age`, which is `dist.Normal(age_mu, age_sigma)`, to generate missing values for the test set:\n",
    "```python\n",
    "def model(age, ...):\n",
    "    ...\n",
    "    age_isnan = jnp.isnan(age)\n",
    "    age_nanidx = jnp.nonzero(age_isnan)[0]\n",
    "    age_impute = numpyro.sample(\"age_impute\", dist.Normal(age_mu[age_nanidx], age_sigma[age_nanidx]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All ingredients are ready for making prediction. We just need to perform the same data transformation process as the train set and use `Predictive` utility as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/agconti/kaggle-titanic/master/data/test.csv\"\n",
    ")\n",
    "d = test_df.copy()\n",
    "d[\"Title\"] = (\n",
    "    d.Name.str.split(\", \")\n",
    "    .str.get(1)\n",
    "    .str.split(\" \")\n",
    "    .str.get(0)\n",
    "    .apply(lambda x: x if x in [\"Mr.\", \"Miss.\", \"Mrs.\", \"Master.\"] else \"Misc.\")\n",
    ")\n",
    "test_data = dict(\n",
    "    age=d.Age.pipe(lambda x: (x - age_mean) / age_std).values,\n",
    "    pclass=d.Pclass.values - 1,\n",
    "    title=d.Title.astype(title_cat).cat.codes.values,\n",
    "    sex=(d.Sex == \"male\").astype(int).values,\n",
    "    sibsp=d.SibSp.clip(0, 1).values,\n",
    "    parch=d.Parch.clip(0, 2).values,\n",
    "    embarked=d.Embarked.astype(embarked_cat).cat.codes.values,\n",
    ")\n",
    "\n",
    "survived_probs = Predictive(model, posterior)(random.PRNGKey(2), **test_data)[\"probs\"]\n",
    "d[\"Survived\"] = (survived_probs.mean(axis=0) >= 0.5).astype(jnp.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl8AAAGACAYAAACTPwd6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3gU5eL28TuFIJBIM2IBokapkUPRICUqIFJCNUACGPCAoD8FpXikt9ASRVTAAorAAUUULCDKOTSlR+CISBMFiSAQIgRIAVL2ef/gzZqQsgGzk03y/VyX1+XOzs7c7M5O7n12dsbNGGMEAAAAS7gXdgAAAICShPIFAABgIcoXAACAhShfAAAAFqJ8AQAAWIjyBQAAYCHKVwmVnp6uBQsW6IknnlDnzp3Vvn17vfrqq0pJSZEkjRw5UvPnzy/klH/Zu3evxo8fX9gx7JYuXap58+YV2PJiY2P17LPPKj9nfomNjVVYWJgk6fjx4xo8eLAk6cSJE2rQoEG+1vf1118rODhYnTp10sCBA/XHH3/Y75s7d67atm2r1q1ba/bs2fZMGzZs0OOPP64OHTrop59+ss8/ZswYbdu2Lc/1HTlyRIMHD1bHjh3VqVMnPfnkk9q1a5f9/vDwcK1Zs0aSFBkZqejo6Hz9OzLbsmWLWrRooW7duuny5cs5zvPmm28qIiIiy7QnnnhC7du3V+fOndW5c2e9//77173uG7FmzRqFh4dbsq6S6KefftILL7xwXY/JvJ+Jjo5Whw4d/laGzO/PG5GQkKA+ffrka95nnnlGn332WYEtD87lWdgBUDgmTpyoCxcuaNGiRfLx8VFycrJeeukljRkzRq+++mphx8vm119/VWxsbGHHsOvZs2eBLm/s2LEaPHiw3NzcHM5bpUoVffzxx5KkkydP6rfffruudR07dkwTJkzQkiVLVLNmTe3cuVMvvPCCVqxYoe+++07ffPONPvvsM3l4eKh///7y9/dX+/btNXv2bC1ZskQnT57Ue++9p1mzZmnv3r1KTExU06ZNc13f0aNH1bdvX02fPl1BQUGSpO3bt+vZZ5/V0qVLdd9992WZ//nnn1evXr306aef6qabbsr3v2v16tXq3r27nnvuuWz3nT59WtOmTdOmTZv0xBNP2KcnJyfr999/1/bt21WqVKl8rwuu7/7779esWbOu6zEFvZ+5kfdnZhcuXMjyQefvKujl4cZRvkqgEydOaNWqVdqyZYu8vb0lSWXLltWkSZP0v//9zz7fDz/8oLCwMP3555+677779Nprr6ls2bJavny5li1bptTUVF24cEEDBgxQr1699Nlnn2nt2rVyd3dXTEyMbrrpJkVFRcnf319xcXGaMGGCjh49Knd3d4WFhalPnz5KSEjQ1KlTdfjwYaWmpqpJkyZ6+eWX5en516Z56tQpzZo1SwkJCRo1apSmT5+uZcuWafHixXJ3d9ctt9yicePG6e67787y70xKStKoUaMUExMjd3d31a1bVxEREdq5c6cmT56sr776StLVT7gZt2fPnq09e/bozJkzuu+++7R792699dZbCggIkCQNGTJEgYGBOnv2rOLj49WyZUtFRUVp1apVkqSLFy+qVatWWrdunS5fvqyIiAidOnVKqampCg4O1rPPPpvt9fjxxx919uxZ1atXT+vWrdMHH3ygjz76SJLUpk0bBQcH64UXXtDp06fVrVs3LV26VJ06ddKuXbs0duxYxcbGqn///po0aZLS09M1fvx4/fTTT0pISNC//vUvtWnTJsv6Dh06pFq1aqlmzZqSpAcffFB//PGHTpw4obVr16pDhw4qW7aspKujQitXrlT79u3l5eWl5ORkJSQkqFSpUjLG6NVXX1VUVFSe29t7772nkJAQe/GSpCZNmui1117LsVz5+PioQYMGWrZsmfr27ZvlvtTUVEVGRmr79u3y8PBQvXr1NGrUKH388cdav369SpcurYSEBI0YMSLL45YvX67AwED5+/vrwoUL9ul79+5V2bJl9fTTT+vcuXNq0qSJhg0bli1X5u2iZs2aGjlypMaPH6+zZ88qLi5Od955p9544w1VrlxZLVu2VNeuXbV9+3adOnVKnTt31pAhQyRdHXlbtWqVKlSoID8/P/vyExISNGnSJB06dEhubm4KCgrSsGHD5Onpqfvvv1///Oc/tW3bNiUnJ2vQoEFas2aNDh8+rFtvvVXvvvuu/fXKMHLkSJUuXVqHDh3S2bNn1axZM40dO1alSpXK9f0bFxenESNGKD4+XpL0yCOPaMiQIblOl6RPP/1US5culc1mU4UKFTRu3Dj5+/tr5MiR8vb21s8//6zTp0+rZs2aioqKUrly5fTdd99pxowZcnd3V+3atbVt2zZ99NFHqlq1ap7LO3/+vI4fP65HH31ULVq0UGRkpGw2m6Sroz7XbueZ39d55clw7X6mS5cuSk5O1tChQ3X06FFduXJFU6ZM0QMPPKCUlBTNmDFDO3fuVHp6uurUqaOxY8fa96fS1W8XMr8/58+fr//973+aMWOGLl26JHd3dw0aNEgtWrTI9TkeNWqULl++rM6dO9s/EGWIjY3VyJEjdebMGd1xxx06e/Zslu09p9f42uV9/vnnOc4HCxiUOGvWrDEhISF5zjNixAjTrVs3k5ycbNLS0kzXrl3N559/bhITE02PHj3MuXPnjDHG/PDDD6Z+/frGGGNWrFhhGjVqZE6dOmWMMSYiIsK8/PLLxhhjnn/+eRMVFWWMMebixYsmODjYHDt2zIwcOdL8+9//NsYYk5aWZl566SUzb968bHlWrFhhBg4caIwxZtu2beaxxx4zZ8+etd/Xrl07Y7PZsjzm888/N/369bMve8yYMebYsWNmx44dJjg42D5f5tuzZs0ybdq0MampqcYYY958800zadIkY4wx58+fN4GBgebixYtm1qxZZtKkScZms5kWLVqYvXv3GmOM+fDDD83w4cONMcaEh4eb9evXG2OMuXz5sgkPDzerV6/O9m+LjIw0s2bNMsYYc+nSJdOwYUNz4cIFc/z4cdOsWTMTGhpqjDFmyZIlZsKECeb48eP25zxz9uPHj5saNWqYNWvWGGOM+e9//2tatWqVbX0xMTEmMDDQHDhwwBhjzPr1603NmjXN//73P9OvXz/z1Vdf2efdunWr6dKlizHGmJ07d5quXbuasLAw8+uvv5ply5aZOXPmZFv+tTp06GC+/fbbPOd58sknzTfffGO//Z///Mf07t0723xvvvmmGTRokElJSTHp6elm5MiRZty4ccaYq9vs+++/n+d6Ml63DOvWrTMvvfSSiY+PN5cvXzaDBg0yU6ZMyfFxmbeLhQsXmrlz5xpjjLHZbObpp5828+fPN8YY06JFCxMZGWmMMeb06dPm/vvvN7///rtZu3atad++vUlISDCpqalm4MCB5sknnzTGGPPyyy+byZMnG5vNZq5cuWL69etnX36NGjXMokWLjDHGzJ071zRo0MCcPn3apKenm65du5qVK1dmyztixAjTpUsXk5iYaK5cuWJ69+5tFi9enOf7d86cOfbnMikpyQwZMsRcvHgx1+nR0dGmV69eJjk52RhjzObNm03btm3t6w8NDTVXrlwxKSkppkuXLmb58uXm3LlzJjAw0Bw8eNAYY8xnn31matSoYY4fP+5weX379rX/+/r06WPfTg8ePGgmTpyY7TnI/N7ILc+1Mu9nduzYYWrXrm327NljjDFmwYIFpk+fPsYYY2bPnm0iIyPt+5zXXnvNTJgwIc8M58+fN48//rg5fvy4MebqtvHwww+bP/74I9fnOPN7/VrPPfecef31140xxhw7dszUr1/frFixIs/XOPPy8poPzsfIVwnk7u5u/8SYl8cee0xlypSRJN133306d+6cypUrp3fffVffffedjh07pkOHDik5Odn+mLp16+q2226TJNWpU0dr166VJG3btk3/+te/JF0d2cgYdfr222/1008/afny5ZKU67E6mW3evFnt27dXpUqVJF0dnZk6dapOnDihatWq2edr1KiRXn/9dYWHh6tp06bq27ev/Pz8dPr06TyXX79+ffvIW0hIiLp166aRI0fqq6++UsuWLeXj42Of183NTSEhIfr88891//3367PPPtPLL7+s5ORk7dy5UxcuXNCbb74p6epXXIcOHVL79u2zrO/o0aP2aTfddJOaNm2qrVu3Kj4+XqGhoVq2bJkSEhK0YcMGPf3003lmL1WqlH0EoFatWlk+DWeoXr26pk2bpgkTJiglJUWtWrVSrVq17KNZmb/6NMbI3f3qoaEPPPCA/ZiSCxcu6NNPP9WSJUv07rvvavfu3apRo4b9Nc7Mzc0tX9tbZlWrVs3x65pNmzZp6NCh9q8Iw8PD9fzzz1/XsjNr1aqVWrVqZb/9zDPPaPDgwRozZky2eTNvF3379tWuXbu0YMECHTt2TL/88ov+8Y9/ZFmudPUr4sqVK+vChQvavn27WrdubR8dCQkJ0eLFi+3/rqVLl8rNzU1eXl4KCwvTokWLNHDgQEmyv6bVq1dXjRo1VKVKFfvzlHkkL7OuXbvaR3Y6d+6s9evX68knn8z1/RsUFKSBAwfq1KlTatq0qYYPHy4fH59cp3/77beKiYmxH38oXR35PX/+vH15Xl5ekqQaNWrowoUL2rVrl/z9/VWrVi17xilTpkiSw+U1atTIPr1du3aKiIjQhg0b1LRpUw0bNizH5yCznPI4Uq1aNfvrWqtWLa1YscKeNSEhwX6sY2pqqipXrpznsvbs2aO4uLgs26ubm5t+/vnnXJ/jvDJu27bNPsLr5+enxo0bS5LDfXSG/M4H56B8lUD16tXT0aNHlZiYmGWYPDY2VuPGjbMfJ5H5qz83NzcZY3T69GmFhoaqR48eatSokdq2bauNGzfa58v8dU3GYzKWlfmP+vHjx1WxYkXZbDa9+eab8vf3l3R1Z+vouKec/pAbY5SWlpZlWrVq1bR27VpFR0drx44d+uc//6mIiAh5e3tnObA9NTU1y+Myf4Vz5513qk6dOvr222/12WefafTo0dnW3a1bN3Xt2lXdu3dXQkKCAgMDlZiYKGOMPv74Y3uBPXfunEqXLp3t8ZmfJ+lq6d20aZMuXryop59+WkePHtW6det0+PBhBQYG6tSpU7k+N5mPW8rteUxJSZGfn58++eQT++1FixapatWquv3223XmzBn7vGfOnLGX6czeeOMNPfvsszp58qS2b9+uRYsWacyYMdq+fbuaNGmSZd769etrz549atGiRZbpc+bMUfXq1dWpU6dsy/f09LSXvsxsNluWf5fNZsv2+l2PDRs2yMfHRw8++KCkq9tR5u0+s8zbxauvvqq9e/cqJCREjRs3VlpaWpbXMPPrnPn1zTxP5q+Qcvp3Zd6eM7+u+T02LfPyM0p0Xu/fevXqaf369dq+fbt27Nih7t2767333st1us1mU+fOne2F22az6cyZMypfvryknPcFHh4e2X5UkvE6O1pe5uc/LCxMLVq00NatW7V582bNmTNHa9asyfH9lSG3fVNern0/ZTzGZrNp9OjReuSRRyRdPcThypUreS4rPT1d/v7++vTTT+3TYmNjValSJZUqVSrH57hChQq5Lu/af0PGdutoH50hv/PBOfi1YwlUpUoVdezYUaNHj1ZiYqIkKTExURMnTlSFChXyPMh53759qlSpkp577jk1b97c/mZNT0/Pc51NmjSxf2pMSEhQ3759dezYMTVv3lwLFy6UMUYpKSn6v//7Py1ZsiTb4z08POx/jIKCgvT111/r3LlzkqQVK1ZkO4ZGkj766CONGjVKzZs317/+9S81b95cBw4cUKVKlXTy5EmdPXtWxhitXr06z+w9evTQe++9p0uXLmX59J2hSpUqqlevnsaPH69u3bpJkry9vVW/fn0tWLBA0tVS2bNnT61fvz7b4++++279/vvv9tstW7bU9u3bdfDgQdWrV0/NmjXTm2++qYcffjjLH9SM5+V6y0dKSop69uxpL3ELFy5Uo0aNVKFCBbVq1UorV65UcnKyUlJS9Nlnn+mxxx7L8vhDhw7p1KlTatWqlVJSUuw7fXd3d126dCnb+vr3769PP/1UW7ZssU/btGmTFi9ebB8BudaJEyd0zz33ZJseFBSkpUuXKjU1VTabTR9++KGaNWt2Xf/+zE6fPq2oqChdvnxZ6enpWrhwYbaRyZxs2bJFffv2VZcuXVS5cmVt27bN4Xvg4Ycf1po1a3Tx4kXZbDZ9+eWX9vuaN2+uJUuW2N8Hn3zySZ4/YsiPb775RikpKbpy5Yo+//xztWjRIs/374wZM/T222/rscce05gxY3Tvvffql19+yXV68+bNtXr1antZX7p0abZj9K7VsGFD+yiLJP3nP/+xf+C6nuWFhYXp4MGDeuKJJzR58mRdvHhRcXFxf+v5krLuZ/LSvHlzffjhh0pJSZHNZtO4ceM0c+bMHJeX8f6sX7++YmJitHPnTknSwYMH1aZNG8XGxub6HHt6eio9PT3HohgUFKRly5ZJunpgf8YvhPN6jTMv70b35SgYjHyVUBMmTNDbb7+tsLAweXh4KCUlRY899pjDn0U3a9ZMy5cvV9u2beXm5qbAwEBVqlRJMTExeT5u/Pjxmjhxojp27ChjjJ555hkFBARozJgxmjp1qjp27KjU1FQ1bdo0x6/W6tevr7feekuDBg3SnDlz9NRTT6lv376y2WyqVKmS5s6dm22kpEuXLvr+++/Vvn17lSlTRrfffrvCw8NVvnx5hYWFKSQkRL6+vnr00Ufz/AVQy5YtNWnSJA0YMCDXebp3764XX3xR77zzjn3ajBkzNHnyZHXs2FEpKSnq0KFDjqM8bdq00dSpU+0/i/fx8ZG/v7/KlCkjDw8PBQUFacyYMXr88cezPfbee+9V6dKl1a1bN73++uu55svM29tbkydP1oABA+yfxqdPn27/tx4+fFjdu3dXamqqWrVqpS5dumR5fGRkpCZMmCBJqlmzpipXrqzWrVvr3nvvzXJQfQY/Pz+9++67euONNxQVFWV/zd555x3VqFEjx4ybN29W27Zts03/v//7P0VFRalLly5KS0tTvXr1NG7cuHz9u3MSFham48ePq2vXrkpPT1fjxo3z9TXm888/r1deeUVvvvmmSpUqpYYNG2Yp0Dl55JFH9PPPPyskJEQ333yzatWqZT/AeuzYsZoyZYr9fRAUFJTjjzOux0033aRevXrp4sWLatOmjUJCQnTlypVc3799+/bVyJEj1aFDB3l5ealmzZoKDg7WhQsXcpzu5eWlAQMGqF+/fnJzc5O3t7fmzJmT58h1hQoVNHPmTI0YMULu7u4KCAiQp6enypQpo+bNm+d7eS+99JKmTZumN954Q25ubho0aJCqVq36t54vKet+Jq/TgDz33HOKioqybze1a9fWyJEjs82X+f356aefatasWXrllVd05coVGWP0yiuvqGrVqrk+9xk/KgkODtaHH36oihUr2pc9YcIEjRo1Su3atdNtt91m/yCT1z7az8/PvrwFCxaoSpUqOc6X0wcfFCw3k5+xVwBO1b9/f7344ouqV69eYUcpdImJiQoLC9OKFSvy/BoJuRs5cqTuu+8+9e/fv7CjZJGYmKi3335bgwcPVpkyZbR//34988wz2rx5c75OswIUF4x8AS5g0qRJmjx5st59990S/0do9uzZGj16NMWrGPL29lapUqXUrVs3eXp6ytPT0z56BZQkjHwBAABYiAPuAQAALET5AgAAsBDlCwAAwEJF5oD7uLgES9ZTsWJZxce7zll+yZM38jjmapnIkzdXyyO5Xiby5M3V8kiul8mKPL6+Prnex8jXNTw9PRzPZCHy5I08jrlaJvLkzdXySK6XiTx5c7U8kutlKuw8lC8AAAALUb4AAAAsRPkCAACwEOULAADAQpQvAAAAC1G+AAAALET5AgAAsJBTy9ePP/6o8PDwbNM3bNigkJAQhYaG6pNPPnFmBAAAAJfitDPcv/fee1q5cqXKlCmTZXpqaqqmT5+u5cuXq0yZMurZs6datGghX19fZ0UBAABwGU4b+apevbpmz56dbfqRI0dUvXp1lS9fXl5eXmrUqJF27drlrBgAAAAuxWkjX23atNGJEyeyTU9MTJSPz1/XOypXrpwSExMdLq9ixbKWXQ4gr+sxFQby5I08jrlaJvLkzdXySK6XiTx5c7U8kutlKsw8ll9Y29vbW0lJSfbbSUlJWcpYbqy6IKevr49lF/HOD/LkjTyOuVom8uTN1fJIrpeJPHlztTyS62WyIo9LXVjb399fMTExOn/+vFJSUrRr1y41aNDA6hgAABSIgICAwo6AIsayka9Vq1YpOTlZoaGhGjlypPr37y9jjEJCQlSlShWrYgAAUKD2799f2BFQxDi1fFWtWtV+KomOHTvap7ds2VItW7Z05qoBAHCq6AOxWr39mCRp/PxoBTe5S43rMJgAxyw/5gsAgKIu+kCs5q78a8TrRFyS/TYFDI5whnsAAK5TxohX9ukxluZA0UT5AgDgOp38M+df4J86m5TjdCAzyhcAANfpjlvK5jj99srlLE6CoojyBQDAdQpuclcu0/2sDYIiiQPuAQC4ThkH1Wcc41XV11vBTfw42B75wsgXgBKNE2TiRjWuU0UR/QMlSRH9AyleyDfKF4ASjRNkArAa5QsAAMBClC8AAAALUb4AAPgb6tatW9gRUMRQvgAA+Bv27dtX2BFQxFC+AAAALET5AlAiRR+I1fj50ZKk8fOjFX0gtpATuS5OxwEULE6yCqDEiT4Qq7kr/zrFxIm4JPttztWUHafjAAoWI18ASpzV24/lMj3G0hwASibKF4AS5+SfyTlOP3U2yeIkAEoiyheAEueOW8rmOP32yuUsTgKgJKJ8AShxgpvclct0P2uDACiROOAeQImTcVB9xjFeVX29FdzEj4PtAViC8gWgRGpcp4oa16mid0dJEf0DCzsOgBKErx0BAAAsRPkCAACwEOULAADAQpQvAAAAC1G+AAA54vqXgHPwa0cAQDZc/xJwHka+AADZcP1LwHkoXwBKtLp16xZ2BJfE9S8B56F8ASjR9u3bV9gRXBLXvwSch/IFAMiG618CzsMB9wCAbLj+JeA8lC8AQI64/iXgHHztCAAAYCHKFwAAgIUoXwAAABaifAEAAFiI8gUAAGAhyhcAAICFKF8AAAAWonwBAABYiPIFAABgIcoXAACAhShfuC4BAQGFHQEAgCKN8oXrsn///sKOAABAkUb5AgAAsBDlCwAAwEKULwAAAAtRvgAAACxE+QIA5Klu3bqFHQEoVihfAIA87du3r7AjAMUK5QsAAMBClC8AAAALUb4AAAAs5LTyZbPZNH78eIWGhio8PFwxMTFZ7l+5cqW6du2qkJAQffTRR86KAQAA4FI8nbXgdevWKSUlRcuWLdOePXsUGRmpd955x37/K6+8oq+++kply5ZVcHCwgoODVb58eWfFAQAAcAlOK1+7d+9WUFCQJKl+/frZfi1Ts2ZNJSQkyNPTU8YYubm5OSsKAACAy3Ba+UpMTJS3t7f9toeHh9LS0uTpeXWV9913n0JCQlSmTBm1bt1aN998c57Lq1ixrDw9PZwVNwtfXx9L1pNf5MkbeRxztUzkyZur5ZFcLxN58uZqeSTXy1SYeZxWvry9vZWUlGS/bbPZ7MXr0KFD+vbbb7V+/XqVLVtW//rXv/TNN9+oXbt2uS4vPj7ZWVGz8PX1UVxcgiXryg9XyyPJpfK42vPjankk18tEnry5Wh7J9TKRJ2+ulkdyvUxW5Mmr3DntgPuGDRtq06ZNkqQ9e/aoRo0a9vt8fHx00003qXTp0vLw8FClSpV08eJFZ0UBAABwGU4b+WrdurW2bt2qsLAwGWM0bdo0rVq1SsnJyQoNDVVoaKh69eqlUqVKqXr16uratauzogAAALgMp5Uvd3d3RUREZJnm7+9v//+ePXuqZ8+ezlo9AACAS+IkqwAAABaifAEAAFiI8gUAAGAhyhcAuJCAgIDCjgDAyShfAOBC9u/fX9gRADgZ5QsAAMBClC8AAAALUb4AAAAsRPkCAACwEOULAADAQpQvAAAAC1G+AAAALET5AgAAsBDlCwAAwEKULwAAAAtRvgAAACxE+QIAALAQ5QsAAMBClC8AAAALUb4AAAAsRPkCAACwEOULAADAQpQvAAAAC1G+kC/RB2I1fn60JGn8/GhFH4gt5EQAABRNnoUdAK4v+kCs5q7cb799Ii7JfrtxnSqFFQsAgCKJkS84tHr7sVymx1iaAwCA4oDyBYdO/pmc4/RTZ5MsTgIAQNFH+YJDd9xSNsfpt1cuZ3ESAACKPsoXHApuclcu0/2sDQIAQDHAAfdwKOOg+oxjvKr6eiu4iR8H2wMAcAMoX8iXxnWqqHGdKnp3lBTRP7Cw4wAAUGTxtSMAAICFKF8AAAAWonwBAABYiPIFAC6AS3gBJQcH3ANAIeMSXkDJwsgXABQyLuEFlCyULwAoZFzCCyhZKF8AUMi4hBdQslC+AKCQcQkvoGTJV/latWqVXn/9dV26dElffPGFszMBKMYCAgIKO4LLaVynip7pVFdVfb0lXb2E1zOd6nKwPVBMOSxfM2bM0Hfffaf//ve/Sk9P14oVKxQZGWlFNgDF0P79+x3PVAI1rlPFfumuiP6BFC+gGHNYvrZs2aJXX31VpUuXlre3txYsWKBNmzZZkQ0AAKDYcVi+3N2vzuLm5iZJSklJsU8DAADA9XF4ktW2bdtqyJAhunDhghYuXKiVK1eqQ4cOVmQDAAAodhyWr4EDB2rz5s264447dOrUKQ0ePFgtWrSwIhsAAECx47B8Pf/88+rUqZOGDh0qLy8vKzIBAAAUWw4P3urWrZvWrl2rxx9/XGPHjtX3339vRS4AAIBiyeHIV4sWLdSiRQtduXJFGzduVGRkpOLj47Vx40Yr8gEAABQrDsuXJP36669avXq11qxZo9tvv119+vRxdi4AAIBiyWH56tixozw8PNSxY0ctWrRIt956qxW5AAAAiiWH5WvGjBmqWbOmFVkAAACKvVzL17hx4zR58mRNmTLFfhTHFrkAACAASURBVILVzP797387NRgAAEBxlGv5Cg0NlSQNHjzYsjAAAADFXa7lKyAgQJK0cOFCde7cWS1atLiu83zZbDZNnDhRP//8s7y8vDRlyhT5+fnZ79+7d68iIyNljJGvr6/9+pEAAADFmcPzfHXv3v2GzvO1bt06paSkaNmyZRo+fLgiIyPt9xljNG7cOE2fPl1Lly5VUFCQ/vjjjxv/VwAAABQRbsYYk58ZM87zNW/evHyd52v69OmqV6+egoODJUlBQUHavHmzJOno0aOaNGmS/P39dfjwYT3yyCMaMGBAnstLS0uXp6dHfqLCidzc3JTPTQbIEdtQ3nh+gOLPaef5SkxMlLe3t/22h4eH0tLS5Onpqfj4eP3www8aN26c/Pz89OyzzyogIEBNmjTJdXnx8cn5ifq3+fr6KC4uwZJ15Yer5ZHkUnlc7flxtTySa2ZypTw8P4652nNEnry5Wh7J9TJZkcfX1yfX+/J9nq9OnTpd13m+vL29lZSUZL9ts9nk6Xl1dRUqVJCfn5/uvfdeSVdHxfbt25dn+QIAACgO8nXM1xdffKF+/fpd1wlWGzZsqE2bNkmS9uzZoxo1atjvq1atmpKSkhQTEyNJ2rVrl+67777rzQ4AAFDkOBz5+uSTT27ockKtW7fW1q1bFRYWJmOMpk2bplWrVik5OVmhoaGaOnWqhg8fLmOMGjRooEcfffRG8gMAABQpDsvXbbfdpj59+ugf//hHllNBDBo0KM/Hubu7KyIiIss0f39/+/83adJEy5cvv968AAAARZrD8lW/fn0rcgAAAJQIDsuXoxEuAAAA5J/D8lWrVq1s13a89dZb9d133zktFAAAQHHlsHwdOnTI/v+pqalat26d9uzZ49RQAAAAxZXDU01kVqpUKbVr1047duxwVh4AAIBizeHI1xdffGH/f2OMfvnlF/vJUgEAAHB9HLao6OjoLLcrVqyoN954w2mBAAAAijOH5Wv69OlW5AAAACgRcj3m69KlS4qKitLevXslXS1hDRo0UO/evRUbG2tZQAAAgOIk1/I1bdo0Xbp0SXfeeae+++47rVq1Sp9//rl69+6d7cz1AAAAyJ9cv3bcs2ePVq1aJUlav3692rVrp7vuukt33XWX5syZY1lAAACA4iTXkS9397/uio6OVpMmTey3U1NTnZsKAACgmMp15KtChQrau3evkpOTdebMGTVt2lTS1SJ22223WRYQAACgOMm1fI0ePVpDhw7V2bNnNWHCBJUtW1Zvv/22Fi9erLlz51qZEQAAoNjItXzVrFlTX3/9dZZpwcHBCg8Pl4+Pj9ODAUBJVLdu3cKOAMDJruvyQn5+fhSvEo4/DLhR0QdiNX7+1ZM2j58fregDnLImJ/v27SvsCACcjOsE4brs27dPcXEJhR0DRUz0gVjNXbnffvtEXJL9duM6VQorFgAUiusa+QKAG7F6+7FcpsdYmgMAXIHDka9Lly5p9uzZ2rFjh9LT09W4cWMNGTJEZcuWtSIfgGLg5J/JOU4/dTbJ4iQAUPgcjnxFRETo8uXLmjZtmqKiopSWlqYJEyZYkQ1AMXHHLTl/WLu9cjmLkwBA4XM48rV//36tXLnSfnv8+PFq3769U0MBKF6Cm9yV5Zivv6b7FUIaAChcDsuXMUYXL17UzTffLEm6ePGiPDw8nB4MQPGRcVB9xjFeVX29FdzEj4PtAZRIDsvXU089pe7du6tFixaSpA0bNmjAgAFODwageGlcp4oa16mid0dJEf0DCzsOABQah+UrJCREAQEB2rVrl2w2m2bPnq2aNWtakQ0AAKDYcVi+Bg8enK1w9e3bV4sWLXJqMAAAgOIo1/I1aNAgHTx4UGfOnFGrVq3s09PT07mwNgAAwA3KtXxFRkbq/Pnzmjp1qsaOHfvXAzw9VblyZUvCAQAAFDe5li9vb295e3vrnXfesTIPAABAscblhQAAQIkSEBBQqOunfAEAgBJl//7sJ322Ur7K1+7du7V06VKlpKRo586dzs4EAABQbDksX4sWLdIbb7yhhQsXKikpSePHj9f8+fOtyAYAAFDsOCxfn3/+uebPn68yZcqoYsWKWr58uVasWGFFNgAAgGLHYflyd3eXl5eX/Xbp0qW5tiMAAMANcniG+8DAQEVFRenSpUtat26dli1bpoceesiKbAAAAMWOw5Gvl19+WX5+fqpZs6a++OILPfrooxoxYoQV2QAAAIodhyNfly5dUnp6umbNmqXY2Fh9/PHHSk1Nlaenw4cCAADgGg5HvoYPH64zZ85IksqVKyebzaaXX37Z6cEAAACKI4fl6+TJkxo6dKikq5ccGjp0qH7//XenBwMAACiOHJYvNzc3/fzzz/bbR44c4StHAACAG+SwRY0YMUL9+vVTlSpVJEnx8fF65ZVXnB4MAACgOHJYvpo2baqNGzfq8OHD8vT01D333JPlvF8AAADIP4fl648//tCSJUt04cIFGWPs06dPn+7UYAAAAMWRw/I1ZMgQPfDAA3rggQfk5uZmRSYAAIBiy2H5SktL46SqAAAABcThrx0bNWqkDRs2KCUlxYo8AAAAxZrDka81a9ZoyZIlWaa5ubnp4MGDTgsFAABQXDksX1u2bLEiBwAAQIngsHydO3dOK1euVFJSkowxstlsOnHiBOf6AgAAuAEOj/kaMmSIDh48qJUrV+rSpUv6z3/+I3d3hw8DAABADhy2qDNnzigqKkotW7bU448/riVLlujAgQNWZAMAACh2HJav8uXLS5LuvvtuHTp0SBUrVnR6KAAAgOLK4TFfDz30kF544QX7NR7379+vm266yYpsAAAAxY7D8jV06FD9/vvvuvPOOzVz5kzt3LlTgwYNcrhgm82miRMn6ueff5aXl5emTJkiPz+/bPONGzdO5cuX10svvXRj/wIAAIAixOHXjqmpqfrtt9/0xRdf6JdfflGFChW0bds2hwtet26dUlJStGzZMg0fPlyRkZHZ5vn44491+PDhG0sOAABQBDkc+XrxxRcVFxcnf3//LNd27NKlS56P2717t4KCgiRJ9evX1759+7Lc/8MPP+jHH39UaGiojh49eiPZAQAAihyH5evo0aNas2bNdS84MTFR3t7e9tseHh5KS0uTp6enzpw5ozlz5mjOnDn65ptv8rW8ihXLytPT47pz3AhfXx9L1pNf5MkbeRxzpUx169Z1qTySaz0/kuvlkVwvE3ny5mp5JNfLVJh5HJav6tWr6+TJk7rjjjuua8He3t5KSkqy37bZbPL0vLq6NWvWKD4+XgMHDlRcXJwuX76se+65R0888USuy4uPT76u9d8oX18fxcUlWLKu/CBP3sjjmKtl2rdvn0vlcbXnx9XySK6XiTx5c7U8kmtmcnaevMpdruUrPDxcbm5uOnfunDp27KhatWrJw+Ovkad///vfea60YcOG2rhxo9q3b689e/aoRo0a9vv69OmjPn36SJI+++wzHT16NM/iBQAAUFzkWr4GDx78txbcunVrbd26VWFhYTLGaNq0aVq1apWSk5MVGhr6t5YNAABQVOVavgIDA3XhwgWlp6erUqVKkqTvv/9e9957r/12Xtzd3RUREZFlmr+/f7b5GPECAAAlSa6nmjhw4ICCg4Oz/Epx69at6ty5sw4dOmRJOAAAgOIm1/IVFRWl1157TQ8//LB92tChQzVt2rQcz9kFAAAAx3ItXxcvXlTjxo2zTQ8KClJ8fLxTQwEAABRXuZavtLQ02Wy2bNNtNptSU1OdGgoAAKC4yrV8Pfjgg5ozZ0626W+//bYCAgKcGgoAAKC4yvXXjsOGDdPAgQP1xRdfqFatWipdurQOHDigSpUq6Z133rEyIwAAQLGRa/ny9vbWhx9+qB07dujgwYNyd3dX79699cADD1iZDwAAoFjJ8/JCbm5uatKkiZo0aWJVHgAAgGIt12O+AAAAUPAoXwAAABaifAEAAFiI8gUAAGAhyhcAAICFKF8AAAAWony5OK4mAABA8UL5cnH79+8v7AgAAKAAUb4AAAAsRPkCAACwEOULAADAQpQvAAAAC1G+AAAALET5AgAAJUL0gViNnx8tSRo/P1rRB2ILJYdnoawVAADAQtEHYjV35V+nbzoRl2S/3bhOFUuzMPIFAACKvdXbj+UyPcbSHBLlCwAAlAAn/0zOcfqps0kWJ6F8AQCAEuCOW8rmOP32yuUsTkL5AgAAJUBwk7tyme5nbRBxwD0AACgBMg6qzzjGq6qvt4Kb+Fl+sL1E+QIAACVE4zpV1LhOFb07SoroH1hoOfjaEQAAwEKULwAAAAtRvgAAACxE+QIAALAQ5QsAAMBClC8AAAALUb4AAAAsRPkCAACwEOULAADAQpQvAAAAC1G+AAAALET5AgAAsBDlCwAAwEKULwAAAAtRvgAAACxE+QIAALAQ5QsAAMBClC8AAAALUb4AAAAsRPkCAACwEOULAADAQpQvAAAAC1G+AAAALET5QpEWEBBQ2BEAALgulC8XFX0gVuPnR0uSxs+PVvSB2EJO5Jr2799f2BEAALguns5asM1m08SJE/Xzzz/Ly8tLU6ZMkZ+fn/3+r776SosWLZKHh4dq1KihiRMnyt2dLihdLV5zV/5VKk7EJdlvN65TpbBiAQCKgICAAG3cuL2wYyAPTms769atU0pKipYtW6bhw4crMjLSft/ly5f1xhtv6N///rc+/vhjJSYmauPGjc6KUuSs3n4sl+kxluYAABQ9fCPg+pxWvnbv3q2goCBJUv369bVv3z77fV5eXvr4449VpkwZSVJaWppKly7trChFzsk/k3OcfupsksVJAABAQXPa146JiYny9va23/bw8FBaWpo8PT3l7u6uW265RZK0ePFiJScnq1mzZnkur2LFsvL09HBW3Cx8fX0sWU9uqt/mo2OnLmabXq2KT6Fnkwr/+bkWeRxztUzkyZur5ZFcLxN58uZqeSTXy1SYeZxWvry9vZWU9NdIjc1mk6enZ5bbr776qn777TfNnj1bbm5ueS4vPj7n0aCC5uvro7i4BEvWlZs2D1bLcsxX5umFnc0Vnp9ruVIeV3x+WrRo4lLHf7jac0Qex1wtE3kcc7U8JfE5yqvcOe1rx4YNG2rTpk2SpD179qhGjRpZ7h8/fryuXLmit99+2/71I65qXKeKnulUV1V9r44cVvX11jOd6nKwPW4Ix38AgGtx2shX69attXXrVoWFhckYo2nTpmnVqlVKTk5WQECAli9frgceeEB9+/aVJPXp00etW7d2Vpwip3GdKmpcp4reHSVF9A8s7DgAAKCAOK18ubu7KyIiIss0f39/+/8fOnTIWasGAABwWZxYCwAAwEKUL6AAcbkjAIAjlC+gAHFwOwDAEcoXAACAhShfAAAAFqJ8AQAAWIjyBQAAYCHKF4qk6AOxGj8/WpI0fn60og/EFnIiAADyx2knWQWcJfpAbJZrX56IS7Lf5hJMAABXx8gXipzV24/lMj3G0hwAgKKpbt26hbp+yheKnJN/Juc4/dTZJIuTAACKon379hXq+ilfKHLuuKVsjtNvr1zO4iQAAFw/yheKnOAmd+Uy3c/aIAAA3AAOuEeRk3FQfcYxXlV9vRXcxI+D7QEARQLlC0VS4zpV1LhOFb07SoroH1jYcQAAyDe+dgQAALAQ5QsAAMBClC+gmOIqAADgmihfQDGUcRWAE3FXz32WcRUAChhQfLnyB66AgIDCjuBSKF/XYANBccBVAICSxdU/cO3fv9/xTCUI5esabCAoDrgKAFCy8IGraKF8AcUQVwEAShY+cBUtlC+gGOIqAEDJwgeuooXyBRRDjetU0TOd6qqqr7ekq1cBeKZTXa4CABRTfOAqWjjDPVAAog/E2o+5GD8/WsFN7ir0osNVAICSg8uuFS2MfP1/rvoT3bp16xZ2BDjg6r8yAoobfpWes8Z1qtg/aEX0D6R4uTDKl1z7j+e+ffsKOwIc4FdGgLX4VTqKOsqX+ONZlLnCyCC/MgIAXA/Kl/jjWZS5wsggvzICAFwPypf444m/h18ZAQCuB+VL/PHE38NpHQBruOoPo4DrxakmxE908fdxWgfAuTJ+GJUh44dRkthXo8hh5Ov/4ye6AOC6+GFU0cRoZc4Y+QIAuDx+GFX0MFqZO0a+AAAujx9GFT2MVuaO8gUAcHn8MKroYbQyd3ztCBRzrnAiWuDv4odRRc8dt5S1XzkmM0YrGfnKhj9UKG5c4US0QEHgh1FFC6OVuWPk6xr79u1TXFxCYccAAKBIY7Qyd5QvAECRwjcURQfnQMwZXzsCAIoUvkpHUUf5AgAAsBDlCwCAYoSvZV0f5QsAgGKEr2VdH+ULAADAQpQvoAAx3A8AcITyBRQghvsBAI5QvgAAgFPxrUBWlC8AAOBUfCuQFeULAADAQpQvAAAAC1G+AAAALET5AgAAsBDlCwAAwEJOK182m03jx49XaGiowsPDFRMTk+X+DRs2KCQkRKGhofrkk0+cFQMAAMClOK18rVu3TikpKVq2bJmGDx+uyMhI+32pqamaPn26PvjgAy1evFjLli1TXFycs6IAAAC4DKeVr927dysoKEiSVL9+/Szn+Dhy5IiqV6+u8uXLy8vLS40aNdKuXbucFQUAAMBleDprwYmJifL29rbf9vDwUFpamjw9PZWYmCgfHx/7feXKlVNiYmKey6tYsaw8PT2cFTcLX18fxzNZiDx5I49jrpaJPHlztTyS62UiT95cLY/kepkKM4/Type3t7eSkpLst202mzw9PXO8LykpKUsZy0l8fLJzgl7D19dHcXEJlqwrP8iTN/I45mqZyJM3V8sjuV4m8uTN1fJIrpfJijx5lTunfe3YsGFDbdq0SZK0Z88e1ahRw36fv7+/YmJidP78eaWkpGjXrl1q0KCBs6IAAAC4DKeNfLVu3Vpbt25VWFiYjDGaNm2aVq1apeTkZIWGhmrkyJHq37+/jDEKCQlRlSpVnBUFAADAZTitfLm7uysiIiLLNH9/f/v/t2zZUi1btnTW6gEAAFySmzHGFHYIAACAkoIz3AMAAFiI8gUAAGAhyhcAAICFKF8AAAAWonwBAABYiPIFAABgoRJVvn788UeFh4fbb69du1bDhw/PNt8777yjoUOH2m+//vrr6t69u3r06KG9e/c6Jc/BgwfVq1cvhYeHq3///vrzzz918OBBhYeH2/+7//77tWnTJqWnp2vKlCkKCwvTE088oY0bNxZqngxHjhxRo0aNdOXKlQLJI0kpKSkaPny4evTooX79+unYsWOKiYlRz5491atXL02YMEE2m02SNG/ePHXu3Fm9e/cusOfkWo62oe3btys0NFS9e/fWCy+8oEuXLkmSnn32WYWFhSk8PFxPP/10gWZKTU3V8OHDFRYWpl69eunIkSPav3+/unXrpl69emny5MmWPkfXk+fDDz9USEiIunXr5pQ8jrZpSfruu+/Uo0cP9ejRQxMnTpQxRvPmzbNv5507d1azZs0KLFNO2/TQoUPt62vZsmWW/c+5c+f0+OOPF+j7Kj+ZDh48qB49eqhnz54aNWqU/TWTrl4u7umnn9bSpUsLPIuj99i2bdv0xBNPqEePHnr99dclSZs2bbI/f08++aRq166tI0eOFHie3PY9OW1Dly9f1uDBg9WrVy8NGDBA586dK/A8uW3TCxcuVPfu3dW9e3fNmTMny+OdsZ929Jrt2rXL/jc0cx5n7hdz2qYzrFq1SqGhofbbOb1+TmdKiHnz5pkOHTqY7t27G2OMmTx5smnTpo0ZMmRIlvm+/fZbExYWZp++f/9+06dPH2Oz2czx48dNx44dnZKnd+/e5sCBA8YYY5YuXWqmTZuWZf6vv/7aDBs2zBhjzIoVK8yECROMMcacPn3aLFiwoFDzGGNMQkKCGTBggHnooYfM5cuX/3aeDIsXLzZjx441xhhz5MgR069fP/PMM8+YHTt2GGOMGTdunPnvf/9rDh06ZDp27GguX75sLl++bLp06WKSk5MLLIcx+duGHn/8cRMXF2eMMWbGjBlm0aJFxhhj2rVrZ2w2W4HmybB27VrzwgsvGGOM2bJlixk0aJDp2rWr2b17tzHGmJkzZ5ovvvjCkufoevKcPXvWtG/f3qSkpJiEhATz8MMPF+hzlJ9tOiEhwQQHB5uzZ8/aH5Px/xkGDhxoNm3aVGC5ctqmM5w/f9506tTJxMbGGmOM2bRpk+ncubNp0KBBgb6v8pPpueeeM99++60xxphhw4aZ9evX2+d/7bXXTLdu3cxHH31UoDny8x7r3Lmz+eWXX4zNZjNhYWHm0KFDWZbx3nvvmddee80peXLa9+S2DX3wwQdm1qxZxhhjvvrqKzN58uQCz5PTNv3777+brl27mrS0NJOenm5CQ0PNwYMHjTHO2U/n5zXr2rWr+f33340xxjz55JNm//79xhjn7hdze58dOHDA9OnTx543P/sAZygxI1/Vq1fX7Nmz7bcbNmyoiRMnZpknJiZGy5Yt0+DBg+3T6tSpo/nz58vNzU0nT57ULbfc4pQ8M2fOVO3atSVJ6enpKl26tP2+5ORkzZ49W2PGjJEkbdmyRbfddpsGDhyosWPHFsiVAv5OHmOMxo0bp2HDhqlMmTJ/O0tmv/76qx5++GFJ0j333GMfRQkMDJQkPfzww9q2bZuOHDmiwMBAlS5dWqVLl5afn59+/vnnAs2Sn21o8eLF9m0kLS1NpUuX1p9//qmLFy/q2WefVc+ePQt8hOfuu+9Wenq6bDabEhMT5enpqdjYWDVs2NCec/fu3ZY8R9eTp1KlSvryyy9VqlQp/fnnn7r55pvl5uZWYDnys03/8MMPqlGjhqKiotSrVy/dcsstqlSpkv0x//3vf3XzzTcrKCiowHLltE1nmD17tp588kndeuutkq5eKWTBggWqUKFCga0/v5lq166t8+fPyxijpKQkeXpevSDKmjVr5ObmZp+/IOXnPZaRKzU1VVeuXJGHh4f9vtOnT+vLL7/UoEGDnJInp31PbtvQ7t277dvNww8/rO3btxd4npy26dtuu03vv/++PDw85O7ubt8POWs/nZ/X7JNPPlG1atWUlJSkxMREVahQwen7xZy26fj4eM2YMUOjR4+2z+doH+AsJaZ8tWnTxr7zkKT27dtn2dEnJSUpIiJCERERWd7MkuTp6anXX39dzzzzjDp06OCUPBk72//9739asmSJnnrqKft9y5cvV9u2be0bRHx8vGJiYjR37lwNGDBAo0aNKtQ8c+bM0SOPPKJatWr97RzXql27tjZu3ChjjPbs2aPY2FgZY+yvXbly5ZSQkKCaNWtq165dSkxMVHx8vH744Qf7V34FxdE2JP31vK1du1bR0dHq0qWLUlNT1a9fP7311luaM2eOpk+frrNnzxZYrrJly+qPP/5Qu3btNG7cOIWHh6tatWr6/vvvJUkbN27UpUuXLHmOriePdPW9tWTJEoWGhqpNmzYFmiM/23R8fLyio6P10ksv6b333tOiRYv022+/2R8zd+7cAvtDniGnbTo9PV1nz57V9u3b9cQTT9jnbdasmSpWrFig689vpurVq2vq1Klq166dzp49q8aNG+vw4cP66quv9OKLLzolR37eYzVr1tSzzz6r9u3b6/bbb9c999xjv2/BggV66qmn5OXl5ZQ8Oe17ctuGEhMT5ePjk2Xegs6T0zZdqlQpVapUScYYRUVFqU6dOrr77rudtp/Oz2vm6empPXv2qGPHjvZy4+z94rXb9KlTpzRq1CiNHj1a5cqVs8/naB/gLCWmfDmydetWxcXFaejQoZo2bZp27NihefPm2e8fOnSoNm/erPnz5+v33393Soavv/5aEyZM0Lx587I071WrVql79+722xUqVNCjjz4qNzc3BQYGZvkuuzDyrFy5UitWrFB4eLji4uLUr1+/AssQEhIib29v9enTRxs3blTdunXl7v7XZpuUlKSbb75Z/v7+6t27twYMGKCoqCj94x//sOSPVk4WLlyo+fPn6/3331fp0qV1yy23KCwsTJ6enqpcubJq165doG/uhQsXqnnz5vrPf/6jL7/8UiNHjtTEiRM1d+5cDRw4UJUrV1bFihUte47ymyfDk08+qc2bN2vnzp3asWNHgefJ7NptukKFCrr//vvl6+urcuXK6YEHHtDBgwclXf3kfPPNN8vPz69AM+S0TXt4eGjNmjXq0KFDtg9/Vsgp0/Tp0/Xhhx9qzZo16tKliyIjI/XFF18oNjZWffv21eeff66FCxdmOe7T2S5evKi5c+dq9erVWrdunfz8/PTBBx9Iunoc2rfffqvg4GCnrT+nfU9u25C3t7eSkpKyzOsMOe2nr1y5opdeeklJSUmaMGGCJOfup/Ojfv362rBhg+rUqaN58+Y5fb947Tbt5uam48ePa+LEiRo2bJh+/fVXTZ06Nc99gDNRvv6/xx9/XCtXrtTixYs1evRoPfTQQxo4cKC2b9+uSZMmSZJKly4tT0/PAv1qJMOXX36pJUuWaPHixapWrZp9ekJCglJSUnT77bfbpzVq1EjfffedJOnQoUNZ7iuMPGvXrtXixYu1ePFi+fr62neGBeGnn35So0aNtHjxYj322GOqVq2a6tSpo+joaElXD7R94IEHdO7cOcXHx2vp0qUaM2aMTp06pfvuu6/AcuTXO++8o127dmnhwoX2HeG2bds0ZMgQSVd3wr/88kuWT+t/180332z/hF2+fHmlpaVp48aNmjZtmubNm6fz58+rWbNmlj1H+c1z9OhRDRo0SMYYlSpVSl5eXln+uBW0nLbpgIAAHT58WOfOnVNaWpp+/PFH3XvvvZKuvm7O+Gotp21auvpjDWes70YzlS9fXt7e3pKujrBcvHhRL7/8sj799FMtXrxYXbt21VNPPWVp5ptuuklly5ZV2bJls+SSpMOHD+vuu+/WTTfd5LT157TvyW0blvsDuQAABwNJREFUatiwoX0/vWnTJjVq1KjA8+S0TRtj9Nxzz6lmzZpZvslx5n46L8YY9erVSxcuXJB0dRTQ3d3d6fvFa7fpdu3aafXq1Vq8eLFmzpype++9V2PGjMlzH+BMno5nKdkCAwO1Zs0ahYWFyWazqXfv3lnKSEFIT0/X1KlTdfvtt9uPN3vwwQf1wgsv6LffftOdd96ZZf4ePXpowoQJ6tGjh4wx9nJYWHmcyc/PT2+++aY++OAD+fj4aOrUqUpOTta4ceM0c+ZM3XPPPWrTpo3c3d114sQJhYSEqFSpUnr55ZctH0H4888/9dZbb6lOnToaMGCAJKldu3bq1auXtmzZoh49esjd3V3Dhg0r0GMKnnrqKY0ePVq9evVSamqqhg4dqnLlymngwIEqU6aMGjdurEceeUTGGEueo/zmkaRatWopNDRUbm5uCgoKsh9PU9Dy2qaHDx9u/6VV27ZtVaNGDUnSb7/9VqC/csyQ0zadsb6C3rf8nUzHjx/X0KFD5enpqVKlSmny5MmFki0zLy8vjRw5Uv369VPp0qXl4+OjyMhISdY8fyNGjMi27/Hw8MhxG6pWrZpGjBihnj17qlSpUnrttdcKNEtu23Tt2rX1/fffKyUlRZs3b5YkDRs2TA0aNCjQ9eeXm5ub+vXrpwEDBsjLy0u+vr6aMmWKypUr59T9Ym7vs2tVqlQp132AM7kZY8VvKgEAACDxtSMAAIClKF8AAAAWonwBAABYiPIFAABgIcoXAACAhTjVBIBCd+LECbVt21b+/v5yc3NTamqqbr31Vk2fPl233XZbYce7bhmXWxk8eLBGjhypHTt2qHz58rLZbPL09NSAAQPUvn37Qk4JoLBQvgC4hFtvvVVffvml/XZkZKReeeUVzZw5sxBTFYwXXnjBftmg48ePq1evXqpQoYKaNm1ayMkAFAbKFwCX1LhxY82cOVPffPPN/2vvXkKibMMwjv9fRwlB84QJ0UARnjaTDoliRh42FaiLEdMIYcJFuxFcJBVDKCqZgUyaBxCboMFFjScQ0iibWgXCgCAKoYGIGiEtDEsdv28xMIug+iSaz+z6beeFe+5ndXG/D+/NwMAAX758YWtri5aWFqxWKwMDAwwNDREREYHFYqGxsZG5uTmcTmdomXBrayvHjx/H5/PhcrnY2dnh2LFjNDU1kZCQQHFxMWVlZbx584bNzU3u3LkT+uJ1Q0MDgUCA06dP4/P5mJyc5OPHjzidTlZXVzEMg/r6evLz87l//35of9yVK1d+2JfZbKampgaPx6PwJfKX0p0vEdl3tre3efbsGVlZWQwODtLT08Po6Ci1tbX09fURCATo7e3l6dOneL1etre3WVtbw+12Y7fb8Xq9VFZW4vf7WV9f5969e/T39zM8PExBQQHt7e2hWvHx8Tx58oSqqip6e3sBaGhowOFwMDIygtlsJhAIANDc3IzNZsPr9dLd3Y3T6WRjYwOAra0txsfHuXz58k/7S0tLY2Fh4TecnIj8CTT5EpF94cOHD5SXlwPBIGOxWKivrycyMpIXL16wuLjI27dviYiIwGQykZ2dTUVFBSUlJdjtdlJSUjh37hyNjY28fv2a4uJiioqK8Pl8rKysUFNTAwQXMMfFxYXqnj17FoDU1FQmJib49OkTy8vLoRVINpuNR48eAcF9jwsLC7hcLgB2dnZYWloCwGKx7Knf37mDUET2N4UvEdkXvr3zBcGFuzabjbKyMnJyckhPT+fx48cAPHjwAL/fj8/no7a2lvb2ds6fP092djYvX77k4cOHTE1NUVhYiNVqpaenB4CvX7/y+fPnUI1Dhw4BwR10ACaTie9tXdvd3cXtdhMfHw8EA2NSUhLPnz/fU5ian5/n5MmT//l5ETlY9NpRRPat9+/fYxgG165dIzc3l8nJSQKBAOvr61y8eJG0tDQcDgdnzpxhfn6euro6ZmZmqKqqwuFwMDs7y6lTp/D7/SwuLgLB0NbW1vbdmrGxsZjNZl69egXA2NhY6Le8vDw8Hg8A7969o7S0lM3NzT335PF4qK6u3utxiMgBocmXiOxbGRkZZGZmcuHCBQzDoKCggOnpaRITE7l06RIVFRVER0dz4sQJbDYbOTk53Lx5k66uLqKiorh9+zbJycm0tLRQV1fH7u4uKSkp3L1794d129rauHHjBh0dHaSnp4emWrdu3cLpdFJaWhp6LiYm5qd9uFwu3G43hmFgMpm4fv06Vqv11w9IRP5Ixj/fm6+LiPylOjs7qays5MiRI0xMTDA2Nhb6dpeIyK/S5EtE5BtHjx7l6tWrREZGcvjwYZqbm//vvyQiB4gmXyIiIiJhpAv3IiIiImGk8CUiIiISRgpfIiIiImGk8CUiIiISRgpfIiIiImGk8CUiIiISRv8CI60qMKf9cM8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_passengers = 15\n",
    "sample_idx = random.choice(\n",
    "    random.PRNGKey(0), range(d.shape[0]), num_passengers, replace=False\n",
    ")\n",
    "sample_idx.sort()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(survived_probs.mean(0)[sample_idx], \"o\")\n",
    "plt.xticks(\n",
    "    ticks=range(num_passengers), labels=d.PassengerId.iloc[sample_idx], rotation=0\n",
    ")\n",
    "\n",
    "p_mu = jnp.mean(survived_probs, axis=0)[sample_idx]\n",
    "p_ci = jnp.percentile(survived_probs, q=(5, 95), axis=0)[:, sample_idx]\n",
    "for i in range(num_passengers):\n",
    "    plt.plot(jnp.repeat(i, 2), p_ci[:, i], \"k\", lw=1)\n",
    "\n",
    "plt.xlabel(\"PassengerID\")\n",
    "plt.ylabel(\"Chance to Survive\")\n",
    "plt.title(\n",
    "    f\"Chance to survive (with 90% CI) of {num_passengers}\"\n",
    "    \" random passengers in the test data\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. McElreath, R. (2016). Statistical Rethinking: A Bayesian Course with Examples in R and Stan.\n",
    "2. Kaggle competition: [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
